{"name":"Docker-scripts","tagline":"Dockerfiles and scripts for Spark and Shark Docker images","body":"# Dockerfiles for Spark and Shark\r\n\r\n## Contents\r\n\r\nDockerfiles to build Spark and Shark images for testing and\r\ndevelopment.\r\n\r\n## Requirements\r\n\r\nTested on Ubuntu 12.04 (Docker version 0.6.4), Ubuntu 13.10 (Docker 0.7.0 and 0.9.0) with the virtual\r\nswitch\r\n\tlxcbr0\r\nenabled. For running Docker on Mac and Windows see [the docs](http://docs.docker.io).\r\nAlso tested inside the VirtualBox Tiny Core Linux VirtualBox VM for Docker on\r\nMac.\r\n\r\nNote: the earlier version of the scripts had problems with newer\r\nversions of Docker (0.7). If you encounter issues please pull the\r\nlatest changes from https://github.com/amplab/docker-scripts.git\r\nmaster branch.\r\n\r\n## Tips for running on Mac OS\r\nIf you are running on Mac OS, installed as described\r\n[in the Docker installation docs](http://docs.docker.io/en/latest/installation/mac/)\r\nyou need to run all commands inside the Docker virtual machine by first ssh-ing into it:\r\n\r\n<pre>\r\n$ ./boot2docker ssh\r\n# User: docker\r\n# Pwd:  tcuser\r\n</pre>\r\n\r\nThen make sure that `python` is installed. Otherwise install it via\r\n`tce-ab` (search for python and install `python.tcz`). Newer versions\r\nof the image that comes with boot2docker also do not have `bash` installed\r\n(install package `bash.tcz`) which is required for the deployment scripts.\r\n\r\nFurther, make sure that your virtual machine running the Docker daemon and\r\nthe containers has sufficient memory allocated (at least 2GB for two Spark worker\r\ncontainers and one master container). This can be done inside the Virtual Box\r\nGUI under the properties of the virtual machine.\r\n\r\nFinally, `boot2docker save` is a good way to perserve changes to the image\r\nbetween restarts of the virtual machine or host computer,\r\nfor example the scripts come in the cloned git repository (see below). \r\n\r\n## Testing\r\n\r\nFirst clone the repository:\r\n\r\n\t$ git clone https://github.com/amplab/docker-scripts.git\r\n\r\nThis repository contains deploy scripts and the sources for the Docker\r\nimage files, which can be easily modified. The main deploy script\r\ntakes the following options.\r\n\r\n<pre>\r\n$ sudo ./deploy/deploy.sh\r\nusage: ./deploy.sh -i &lt;image&gt; [-w &lt;&#35;workers&gt;] [-v &lt;data_directory&gt;] [-c]\r\n\r\n  image:    spark or shark image from:\r\n                 spark:1.0.0  spark:1.1.0\r\n                 shark:0.8.0\r\n</pre>\r\n\r\nThe script either starts a standalone Spark cluster or a standalone\r\nSpark/Shark cluster for a given number of worker nodes. Note that\r\non the first call it may take a while for Docker to download the\r\nvarious images from the repository,\r\n\r\nIn addition to Spark (and Shark) the cluster also runs a Hadoop HDFS\r\nfilesystem. When the deploy script is run it generates one container\r\nfor the master node, one container for each worker node and one extra\r\ncontainer running a Dnsmasq DNS forwarder. The latter one can also be\r\nused to resolve node names on the host, for example to access the\r\nworker logs via the Spark web UI.\r\n\r\nOptionally one can set the number of workers (default: 2) and a data directory\r\nwhich is a local path on the host that can be mounted on the master and\r\nworker containers and will appear under /data.\r\n\r\nBoth the Spark and Shark shells are started in a separate container.\r\nThis container can be directly started from the deploy script by\r\npassing \"-c\" to the deploy script.\r\n\r\nEach node (worker and master) also runs a sshd which is\r\n_pre-configured with the given RSA key_. Note that you should change\r\nthis key if you plan to expose services running inside the containers.\r\nSince the permissions of the key when cloned from the repository are\r\nlikely wrong you need to change them if you intend to log in with ssh:\r\n\r\n<pre>\r\nchmod go-rwx apache-hadoop-hdfs-precise/files/id_rsa\r\n</pre>\r\n\r\n### Example: Running a Spark cluster\r\n\r\nStarting from the directory in which the repository was cloned do\r\n\r\n#### Deploy the cluster\r\n\r\n\t$ sudo ./deploy/deploy.sh -i amplab/spark:1.1.0 -w 3 \r\n\r\n#### Wait a few seconds\r\n\r\nWait for the \"cluster\" to come up. Note that it can take longer to download\r\nthe container images the first time but after that the process is fairly quick.\r\nWhen the cluster comes up you should see something like this:\r\n\r\n<pre>\r\n> sudo ./deploy.sh -i amplab/spark:1.1.0 -w 3 \r\n*** Starting Spark 1.1.0 ***\r\nstarting nameserver container\r\nstarted nameserver container:  069557913d98a37caf43f8238dfdf181aea5ab30eb42e382db83307e277cfa9e\r\nDNS host->IP file mapped:      /tmp/dnsdir_12015/0hosts\r\nNAMESERVER_IP:                 172.17.0.8\r\nwaiting for nameserver to come up \r\nstarting master container\r\nstarted master container:      f50a65d2ef7b17bffed7075ac2de4a7b52c26adff15bdbe14d3280ef4991c9d6\r\nMASTER_IP:                     172.17.0.9\r\nwaiting for master ........\r\nwaiting for nameserver to find master \r\nstarting worker container\r\nstarted worker container:  576d7d223f59a6da7a0e73311d1e082fad27895aef53edf3635264fb00b70258\r\nstarting worker container\r\nstarted worker container:  5672ea896e179b51fe2f1ae5d542c35706528cd3a768ba523324f434bb2b2413\r\nstarting worker container\r\nstarted worker container:  3cdf681f7c99c1e19f7b580ac911e139923e9caca943fd006fb633aac5b20001\r\nwaiting for workers to register .....\r\n\r\n***********************************************************************\r\nstart shell via:            sudo /home/andre/docker-scripts/deploy/start_shell.sh -i amplab/spark-shell:0.9.0 -n 069557913d98a37caf43f8238dfdf181aea5ab30eb42e382db83307e277cfa9e \r\n\r\nvisit Spark WebUI at:       http://172.17.0.9:8080/\r\nvisit Hadoop Namenode at:   http://172.17.0.9:50070\r\nssh into master via:        ssh -i /home/andre/docker-scripts/deploy/../apache-hadoop-hdfs-precise/files/id_rsa -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no root@172.17.0.9\r\n\r\n/data mapped:               \r\n\r\nkill master via:           sudo docker kill f50a65d2ef7b17bffed7075ac2de4a7b52c26adff15bdbe14d3280ef4991c9d6\r\n***********************************************************************\r\n\r\nto enable cluster name resolution add the following line to _the top_ of your host's /etc/resolv.conf:\r\nnameserver 172.17.0.8\r\n</pre>\r\n\r\n#### Start the Spark shell container as shown above, for example:\r\n\r\n\t$ sudo /home/andre/docker-scripts/deploy/start_shell.sh -i amplab/spark-shell:1.1.0 -n 069557913d98a37caf43f8\r\n\r\nThe parameter passed with -n is the ID of the nameserver container.\r\nThen attach to the running shell via the given command, for example:\r\n\r\n    $ sudo docker attach 9ac49b09bf18a13c7\r\n\r\nIf the screen appears to stay blank just hit return to get to the prompt.\r\n\r\n#### Execute an example:\r\n\r\n<pre>\r\nscala&gt; val textFile = sc.textFile(\"hdfs://master:9000/user/hdfs/test.txt\")\r\nscala&gt; textFile.count()\r\nscala&gt; textFile.map({line => line}).collect()\r\n</pre>\r\n\r\n\r\n#### Terminate the cluster:\r\n\r\n\t$ sudo ./deploy/kill_all.sh spark\r\n\t$ sudo ./deploy/kill_all.sh nameserver\r\n\r\n### Shark\r\n\r\nBasically the same steps apply only that the Shark images are chosen instead of the Spark ones\r\n(the former contain in addition to Spark the Shark binaries).\r\n\r\n#### Deploy the cluster\r\n\r\n\t$ sudo ./deploy/deploy.sh -i amplab/shark:0.8.0 -w 3\r\n\r\n#### Wait a few seconds\r\n\r\nWait for the \"cluster\" to come up. Note that it can take longer to download\r\nthe container images the first time but after that the process is fairly quick.\r\nWhen the cluster comes up you should see something like this:\r\n\r\n<pre>\r\n*** Starting Shark 0.8.0 + Spark ***\r\nstarting nameserver container\r\nstarted nameserver container:  952d22e085c3b74e829e006ab536d45d31800c463832e43d8679bbf3d703940e\r\nDNS host->IP file mapped:      /tmp/dnsdir_30578/0hosts\r\nNAMESERVER_IP:                 172.17.0.13\r\nwaiting for nameserver to come up \r\nstarting master container\r\nstarted master container:      169f253eaddadb19b6eb28e79f148eef892f20d34602ffb42d3e57625dc61652\r\nMASTER_IP:                     172.17.0.14\r\nwaiting for master ........\r\nwaiting for nameserver to find master \r\nstarting worker container\r\nstarted worker container:  1c6920c96d5ad684a2f591bfb334323c5854cdd7a0da49982baaf77dc4d62ac7\r\nstarting worker container\r\nstarted worker container:  7250dcfb882e2d17441c8c59361d10d8c59afb2b295719ba35f59bc72c6f17a5\r\nstarting worker container\r\nstarted worker container:  26823e188a2a5a5897ed4b9bf0fca711dc7f98674fe62eb78fb49cf031bec79c\r\nwaiting for workers to register .......\r\n\r\n***********************************************************************\r\nstart shell via:            sudo /home/andre/docker-scripts/deploy/start_shell.sh -i amplab/shark-shell:0.8.0 -n 952d22e085c3b74e829e006ab536d45d31800c463832e43d8679bbf3d703940e \r\n\r\nvisit Spark WebUI at:       http://172.17.0.14:8080/\r\nvisit Hadoop Namenode at:   http://172.17.0.14:50070\r\nssh into master via:        ssh -i /home/andre/docker-scripts/deploy/../apache-hadoop-hdfs-precise/files/id_rsa -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no root@172.17.0.14\r\n\r\n/data mapped:               \r\n\r\nkill master via:           sudo docker kill 169f253eaddadb19b6eb28e79f148eef892f20d34602ffb42d3e57625dc61652\r\n***********************************************************************\r\n\r\nto enable cluster name resolution add the following line to _the top_ of your host's /etc/resolv.conf:\r\nnameserver 172.17.0.13\r\n</pre>\r\n\r\n#### Start the Shark shell container as shown above, for example:\r\n\r\n\t$ sudo /home/andre/docker-scripts/deploy/start_shell.sh -i amplab/shark-shell:0.8.0 -n 952d22e085c3b74e829e00\r\n\r\nThe parameter passed with -n is the ID of the nameserver container.\r\nThen attach to the running shell via the given command, for example:\r\n\r\n    $ sudo docker attach 9ac49b09bf18a13c7\r\n\r\nIf the screen appears to stay blank just hit return to get to the prompt.\r\n\r\n#### Execute an example:\r\n\r\n<pre>\r\nshark> CREATE TABLE src(key INT, value STRING);\r\nshark> LOAD DATA LOCAL INPATH '${env:HIVE_HOME}/examples/files/kv1.txt' INTO TABLE src;\r\nshark> SELECT COUNT(1) FROM src;\r\n</pre>\r\n\r\n#### Terminate the cluster:\r\n\r\n\t$ sudo ./deploy/kill_all.sh shark\r\n\t$ sudo ./deploy/kill_all.sh nameserver\r\n\r\n## Building\r\n\r\nIf you prefer to build the images yourself (or intend to modify them) rather\r\nthan downloading them from the Docker repository, you can build\r\nall Spark and Shark images in the correct order via the build script:\r\n\r\n\t$ ./build/build_all.sh\r\n\r\nThe script builds the images in an order that satisfies the chain of\r\ndependencies:\r\n\r\napache-hadoop-hdfs-precise -> spark-base -> spark-{master, worker, shell}\r\n\r\napache-hadoop-hdfs-precise -> spark-base -> shark-base -> shark-{master, worker, shell}\r\n\r\nYou can always (re-)build single images by cd-ing into the image directory and doing\r\n\r\n\t$ . build\r\n\r\n## Best practices for Dockerfiles and startup scripts\r\n\r\nThe following are just some comments that made the generation of the images easier. It\r\nis not enforced in any way by Docker.\r\n\r\nThe images and startup scripts follow the following structure in order to reuse\r\nas much as possible of the image they depend on. There are two types of images,\r\n<em>base</em> images and <em>leaf</em> images. Leaf images, as the name suggests,\r\nare images that are leafs in the dependency tree. For example, spark-base as a base\r\nimage depends on apache-hadoop-hdfs-precise. spark-master depends on spark-base as\r\nits base image and is itself a leaf.\r\n\r\nIn addition to its Dockerfile, each image has a\r\n\tfiles/\r\nsubdirectory in its image directory that contains files (config files, data files) that will be copied\r\nto the\r\n\troot/<em>image_name</em>_files\r\ndirectory inside the image.\r\n\r\n### Base images\r\n\r\nBase images are images that are intended to be extended by other images and therefore do not\r\nhave a default command or entry point. They are good for testing though, e.g, by running\r\n\t/bin/bash\r\ninside them. \r\n\r\n\r\nFor base images such as spark-base, besides data files the\r\n\tfiles/\r\ndirectory also contains\r\n\tfiles/configure_spark.sh\r\nwhich is a script that contains four functions\r\n\r\n*\tcreate_spark_directories\r\n  for creating required directories such as the working directory\r\n*\tdeploy_spark_files\r\n  that would copy files from\r\n\t/root/<em>image_name</em>_files\r\n  to required system path locations\r\n*\tconfigure_spark\r\n  that changes settings in config files and takes the IP of the master as argument\r\n*\tprepare_spark\r\n  that calls the previous three in the given order and takes the IP of the master as argument\r\n\r\n\r\nAll of the functions of a __base-image__'s configure script, so also inside\r\n\tfiles/configure_spark.sh\r\nexcept __prepare_spark__ first call their corresponding functions in the image the spark-base image depends on (apache-hadoop-hdfs-precise in this case). Therefore all the underlying services get initialized before the top level service. \r\n\r\n### Leaf images\r\n\r\nFor leaf images such as spark-master, besides data files the\r\n\tfiles/\r\ndirectory also contains\r\n\tfiles/default_cmd\r\nthat is chosen in the image's Dockerfile to be the default command (or entry point) to the image. This means the command\r\ninside is executed whenever the container is started.\r\n\r\n\r\nThe default command script executes the following steps in this order\r\n\r\n1. The first thing the default command does is call the prepare\r\n   function of the configure script inside its base image. In this case, the default command script calls function\r\n\tprepare_spark\r\n   inside\r\n\t/root/spark-base/configure_spark.sh\r\nwhich is the location the configure script of spark-base was copied to.\r\n2. After that, now that the base images configuration (and the configuration of the images it inherits from) has completed, the\r\n   default command may start services it relies on, such as the Hadoop namenode service in the case of spark-master.\r\n3. Finally, the default command script of spark-master runs a second script under userid hdfs\r\n   (the Hadoop HDFS super user), which is\r\n\tfiles/files/run_spark_master.sh\r\n   that actually starts the master.\r\n \r\n\r\nThe spark-worker default command proceeds along the same lines but starts a Spark worker with a Hadoop datanode instead.\r\n\r\n## Tips\r\n\r\n### Name resolution on host\r\n\r\nIn order to resolve names (such as \"master\", \"worker1\", etc.) add the IP\r\nof the nameserver container to the top of /etc/resolv.conf on the host.\r\n\r\n### Maintaining local Docker image repository\r\n\r\nAfter a while building and debugging images the local image repository gets\r\nfull of intermediate images that serve no real purpose other than\r\ndebugging a broken build. To remove these do\r\n\r\n\t$ sudo docker images | grep \"<none>\" | awk '{print $3}' | xargs sudo docker rmi\r\n\r\nAlso data from stopped containers tend to accumulate. In order to remove all container data (__only do when no containers are running__) do\r\n\r\n\t$ sudo docker rm `sudo docker ps -a -q`\r\n","google":"UA-25796498-1","note":"Don't delete this file! It's used internally to help with page regeneration."}